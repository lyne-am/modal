{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib.parse\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import spacy\n",
    "import json\n",
    "import networkx as nx\n",
    "from networkx.algorithms import bipartite\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import pathlib\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "    \n",
    "def occurences(url, word):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        return (\"erreur\")\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    paragraphs = soup.find_all('p')\n",
    "    full_text = ' '.join(p.get_text() for p in paragraphs)\n",
    "\n",
    "    occurences = 0\n",
    "    for match in re.finditer(rf'\\b{re.escape(word)}\\b', full_text, re.IGNORECASE):\n",
    "        occurences += 1\n",
    "        print(f\"{occurences} starts at: {match.start()} ends at: {match.end()}\")\n",
    "\n",
    "    return occurences\n",
    "\n",
    "\n",
    "\n",
    "def theme(url, word):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    paragraphs = soup.find_all('p')\n",
    "    full_text = ' '.join(p.get_text() for p in paragraphs)\n",
    "\n",
    "    themes = []\n",
    "    occurences = 0\n",
    "\n",
    "    for match in re.finditer(rf'\\b{re.escape(word)}\\b', full_text, re.IGNORECASE):\n",
    "        occurences += 1\n",
    "        x = match.start()\n",
    "        y = match.end()\n",
    "        \n",
    "        # On selectionne quelques mots qui entourent le mot-clé \n",
    "        sentence = full_text[max(0, x - 70): y]\n",
    "\n",
    "        # On supprime les références wikipedia eventuellement contenue\n",
    "        sentence = re.sub(r'\\[\\d+\\]', '', sentence)  \n",
    "\n",
    "        print(\"> Phrase séléctionnée: \", sentence.strip())\n",
    "\n",
    "        # On selectionne uniquement le fragment de mots contenus autour de notre mot-clé et \n",
    "        # entre 2 signes de ponctuations si presents\n",
    "        fragments = re.split(r'[.?!;,:\\n]', sentence)\n",
    "        fragment = next((frag.strip() for frag in fragments if word.lower() in frag.lower()), None)\n",
    "        print(\"> Fragment: \",fragment)\n",
    "\n",
    "        # On utilise spaCy pour supprimer les \"stopwords\" \n",
    "        doc = nlp(fragment)\n",
    "        without_stopwords = [token.text for token in doc if not token.is_stop]\n",
    "        sentence = ' '.join(without_stopwords)\n",
    "        print(\"> Fragment sans stopwords:\", sentence)\n",
    "\n",
    "        # On extrait le theme, en utilisant spaCy\n",
    "        for mot in doc:\n",
    "            if mot.text.lower() == word.lower():\n",
    "                mots_lies = []\n",
    "                i = mot.i - 1\n",
    "                while i >= 0:\n",
    "                    prev = doc[i]\n",
    "                    if prev.dep_ in (\"amod\", \"compound\") and prev.pos_ in (\"ADJ\", \"NOUN\"):\n",
    "                        mots_lies.insert(0, prev.text)\n",
    "                        i -= 1\n",
    "                    else:\n",
    "                        break\n",
    "\n",
    "                theme = ' '.join(w.lower() for w in mots_lies + [mot.text])\n",
    "                print(\"> Theme: \", theme)\n",
    "                themes.append(theme)\n",
    "                break  \n",
    "\n",
    "    return themes\n",
    "\n",
    "\n",
    "\n",
    "def wikidata_to_wikipedia(wikidata_url, lang='en'):\n",
    "    entity_id = wikidata_url.strip().split('/')[-1]\n",
    "    api_url = 'https://www.wikidata.org/w/api.php'\n",
    "    params = {'action': 'wbgetentities', 'ids': entity_id, 'format': 'json', 'props': 'sitelinks'}\n",
    "\n",
    "    response = requests.get(api_url, params=params)\n",
    "    data = response.json()\n",
    "\n",
    "    sitelinks = data['entities'].get(entity_id, {}).get('sitelinks', {})\n",
    "    wiki_key = f'{lang}wiki'\n",
    "    if wiki_key in sitelinks:\n",
    "        title = sitelinks[wiki_key].get('title')\n",
    "        if title:\n",
    "            encoded_title = urllib.parse.quote(title.replace(' ', '_'))\n",
    "            return f'https://{lang}.wikipedia.org/wiki/{encoded_title}'\n",
    "    return f\"Non trouvee.\"\n",
    "\n",
    "\n",
    "#theme(\"https://en.wikipedia.org/wiki/Rapha%C3%ABl_Coleman\",\"activist\")\n",
    "#wikidata_to_wikipedia(\"http://www.wikidata.org/entity/Q240573\", lang='en')\n",
    "\n",
    "# G1 = nx.Graph()\n",
    "# artistes = json.loads(pathlib.Path(\"artistes.json\").read_text())\n",
    "with open(\"artistes.json\", \"r\", encoding=\"utf-8\") as f:artistes = json.load(f)\n",
    "# for artiste in artistes:\n",
    "#     nom = artiste.get(\"personLabel\")\n",
    "#     wikidata_url = artiste.get(\"person\")\n",
    "#     wikipedia_url = wikidata_to_wikipedia(wikidata_url)\n",
    "#     if wikipedia_url.startswith(\"http\"):\n",
    "#         themes_extraits = theme(wikipedia_url, \"activist\")\n",
    "#         if themes_extraits:\n",
    "#             G1.add_node(nom, bipartite=0)\n",
    "#             for t in themes_extraits:\n",
    "#                 G1.add_node(t, bipartite=1)\n",
    "#                 G1.add_edge(nom, t)\n",
    "\n",
    "\n",
    "# left_nodes = {n for n, d in G1.nodes(data=True) if d.get(\"bipartite\") == 0}\n",
    "# right_nodes = {n for n, d in G1.nodes(data=True) if d.get(\"bipartite\") == 1}\n",
    "# pos = nx.bipartite_layout(G1, left_nodes)\n",
    "# plt.figure(figsize=(14, 10))\n",
    "# nx.draw_networkx(\n",
    "#     G1, pos,\n",
    "#     node_color=[\"skyblue\" if n in left_nodes else \"lightgreen\" for n in G1.nodes()],\n",
    "#     node_size=1200,\n",
    "#     font_size=9,\n",
    "#     with_labels=True,\n",
    "#     edge_color=\"gray\"\n",
    "# )\n",
    "# plt.show()\n",
    "\n",
    "# for theme in right_nodes:\n",
    "#     connected_artists = list(G1.neighbors(theme))\n",
    "#     print(f\"Le thème '{theme}' est relié à {len(connected_artists)} artiste(s)\")\n",
    "\n",
    "\n",
    "communautes = {\"climate\" :[\"climate activist\", \"environmental activist\", \"climate change activist\", \"anti - nuclear activist\"],\n",
    "               \"animal rights\":[\"animal rights activist\", \"vegan activist\"],\n",
    "                \"human rights\" : [\"human rights activist\", \"civil rights activist\"],\n",
    "                \"women & lgbt rights\" : [\"feminist activist\", \"gay activist\", \"lesbian activist\", \"transgender activist\", \"sexual purity activist\"],\n",
    "                \"politics\" : [\"political activist\"],\n",
    "                \"social\":[\"social activist\", \"social media activist\", \"yellow vest activist\"],\n",
    "                \"health\" : [\"cancer research activist\"]}\n",
    "\n",
    "# G2 = nx.Graph()\n",
    "# artiste_to_themes = {}\n",
    "# for artiste in artistes:\n",
    "#     nom = artiste.get(\"personLabel\")\n",
    "#     wikidata_url = artiste.get(\"person\")\n",
    "#     wikipedia_url = wikidata_to_wikipedia(wikidata_url)\n",
    "#     if wikipedia_url.startswith(\"http\"):\n",
    "#         themes_extraits = theme(wikipedia_url, \"activist\")\n",
    "#         if themes_extraits:\n",
    "#             artiste_ajoute = False\n",
    "#             for t in themes_extraits:\n",
    "#                 for communaute, theme2 in communautes.items():\n",
    "#                     for i in range (len(theme2)):    \n",
    "#                         if theme2[i] in t: \n",
    "#                             if artiste_ajoute == False:\n",
    "#                                 G2.add_node(nom, bipartite=0) \n",
    "#                                 artiste_ajoute = True\n",
    "#                                 G2.add_node(communaute, bipartite=1)\n",
    "#                                 G2.add_edge(nom, communaute)\n",
    "#                                 artiste_to_themes.setdefault(nom, set()).add(communaute)\n",
    "#                                 break\n",
    "#                             else:\n",
    "#                                 G2.add_node(communaute, bipartite=1)\n",
    "#                                 G2.add_edge(nom, communaute)\n",
    "#                                 artiste_to_themes.setdefault(nom, set()).add(communaute)\n",
    "#                                 break\n",
    "\n",
    "\n",
    "# left_nodes = {n for n, d in G2.nodes(data=True) if d.get(\"bipartite\") == 0}\n",
    "# right_nodes = {n for n, d in G2.nodes(data=True) if d.get(\"bipartite\") == 1}\n",
    "# pos = nx.bipartite_layout(G2, left_nodes, align='vertical', scale=5)\n",
    "# plt.figure(figsize=(14, 10))\n",
    "# nx.draw_networkx(\n",
    "#     G2, pos,\n",
    "#     node_color=[\"skyblue\" if n in left_nodes else \"lightgreen\" for n in G2.nodes()],\n",
    "#     node_size=800,\n",
    "#     font_size=7,\n",
    "#     with_labels=True,\n",
    "#     edge_color=\"gray\"\n",
    "# )\n",
    "# plt.show()\n",
    "\n",
    "# with open(\"chanteurs.json\", \"r\", encoding=\"utf-8\") as f:chanteurs = json.load(f)\n",
    "# chanteurs_activists = {}\n",
    "# for personne in artiste_to_themes:\n",
    "#     for chanteur in chanteurs:\n",
    "#         if personne == chanteur.get(\"personLabel\"):\n",
    "#             chanteurs_activists[personne] = list(artiste_to_themes[personne])\n",
    "# print (chanteurs_activists)\n",
    "\n",
    "def related_words(word):\n",
    "    related = set()\n",
    "    for syn in wn.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            related.add(lemma.name())\n",
    "        for hyper in syn.hypernyms():\n",
    "            for lemma in hyper.lemmas():\n",
    "                related.add(lemma.name())\n",
    "        for hypo in syn.hyponyms():\n",
    "            for lemma in hypo.lemmas():\n",
    "                related.add(lemma.name())\n",
    "    return related\n",
    "\n",
    "# On commence maintenant l'analyse des chansons \n",
    "import lyricsgenius\n",
    "genius = lyricsgenius.Genius(\"Fu9F4P_Zft__NzCvberMcG773LOGTEU1bpqA9cdGErZYU41AWnwlmlr649k55FRD\", timeout=15)\n",
    "genius.remove_section_headers = True # Remove section headers (e.g. [Chorus]) from lyrics when searching\n",
    "genius.excluded_terms = [\"(Remix)\", \"(Live)\", \"(Edit)\", \"(Version)\"] # Exclude songs with these words in their title\n",
    "\n",
    "#pour obtenir les paroles d'une chanson\n",
    "def paroles(chanson, chanteur):\n",
    "    song = genius.search_song(chanson, chanteur)\n",
    "    return song.lyrics\n",
    "\n",
    "#pour obtenir les titres des chansons d'un artiste\n",
    "def chansons(chanteur, max_chansons):\n",
    "    artiste = genius.search_artist(chanteur, max_songs=max_chansons, sort=\"popularity\")\n",
    "    return [chanson.title for chanson in artiste.songs]\n",
    "\n",
    "def texte_paroles(texte):\n",
    "    texte = texte.lower()\n",
    "    texte = texte.replace('-', '')\n",
    "    texte = texte.translate(str.maketrans('', '', string.punctuation))\n",
    "    return set(word_tokenize(texte))\n",
    "\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "model_path = \"GoogleNews-vectors-negative300.bin\"\n",
    "model = KeyedVectors.load_word2vec_format(model_path, binary=True,limit=500000)\n",
    "\n",
    "def nettoyer_mot(mot):\n",
    "    mot = mot.replace(\"_\", \" \")          \n",
    "    mot = mot.lower()                     \n",
    "    mot = re.sub(r\"[^\\w\\s]\", \"\", mot)     \n",
    "    return mot.strip()\n",
    "\n",
    "def related_words2(keyword, seuil=0.3):\n",
    "    try:\n",
    "        similar = model.most_similar(keyword, topn=5000000)  \n",
    "        mots = set([word for word, score in similar if score >= seuil])\n",
    "        return set(nettoyer_mot(mot) for mot in mots)\n",
    "    except KeyError:\n",
    "        return set()\n",
    "    \n",
    "\n",
    "#attribue un score de similarité entre les paroles d'une chanson et un theme\n",
    "def score_similarite(chanson, chanteur, theme):\n",
    "    mots_lies = related_words2(theme)\n",
    "    lyrics = paroles(chanson, chanteur)\n",
    "    mots_lyrics = texte_paroles(lyrics)\n",
    "    mots_communs = mots_lyrics.intersection(mots_lies)\n",
    "    print(mots_communs)\n",
    "    #return len(mots_communs) / max(1, len(mots_lyrics))\n",
    "    if len(mots_communs) >= 20:\n",
    "        return \"A+\"\n",
    "    if 15 <= len(mots_communs) <= 20:\n",
    "        return \"A\"\n",
    "    if 10<=len(mots_communs) <= 15:\n",
    "        return \"B\"\n",
    "    if 5<= len(mots_communs) <= 10:\n",
    "        return \"C\"\n",
    "    if 0 < len (mots_communs)<= 5:\n",
    "        return \"D\"\n",
    "    else: \n",
    "        return \"E\"\n",
    "\n",
    "chanteurs_themes = {'M.I.A.': ['politics'], 'KRS-One': ['human rights', 'animal rights'], 'Killer Mike': ['social'], 'Michael Cuccione': ['health'], 'L7a9d': ['human rights'], 'Saba Saba': ['social'], 'Rebecca Moore': ['animal rights'], 'Joaquin Phoenix': ['animal rights'], 'Wyclef Jean': ['politics'], 'Adé Bantu': ['social'], 'Tupac Shakur': ['politics', 'human rights'], 'Mai Khôi': ['politics'], 'Jacline Mouraud': ['social'], 'Madeleina Kay': ['politics'], 'Topher': ['politics'], 'Montana Tucker': ['social']}\n",
    "# for chanteur, themes_associes in chanteurs_themes.items():\n",
    "#     songs = chansons(chanteur, 5)  \n",
    "#     for theme in themes_associes:\n",
    "#         scores = []\n",
    "#         for chanson in songs:\n",
    "#             score_chanson = score_similarite(chanson, chanteur, theme)\n",
    "#             scores.append(score_chanson)\n",
    "#         print (chanteur, theme, scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

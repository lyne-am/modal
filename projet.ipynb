{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib.parse\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import spacy\n",
    "import json\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from networkx.algorithms import bipartite\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import pathlib\n",
    "import sklearn\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "from itertools import combinations\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "    \n",
    "def occurences(url, word):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        return (\"erreur\")\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    paragraphs = soup.find_all('p')\n",
    "    full_text = ' '.join(p.get_text() for p in paragraphs)\n",
    "\n",
    "    occurences = 0\n",
    "    for match in re.finditer(rf'\\b{re.escape(word)}\\b', full_text, re.IGNORECASE):\n",
    "        occurences += 1\n",
    "        print(f\"{occurences} starts at: {match.start()} ends at: {match.end()}\")\n",
    "\n",
    "    return occurences\n",
    "\n",
    "\n",
    "\n",
    "def theme(url, word):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    paragraphs = soup.find_all('p')\n",
    "    full_text = ' '.join(p.get_text() for p in paragraphs)\n",
    "\n",
    "    themes = []\n",
    "    occurences = 0\n",
    "\n",
    "    for match in re.finditer(rf'\\b{re.escape(word)}\\b', full_text, re.IGNORECASE):\n",
    "        occurences += 1\n",
    "        x = match.start()\n",
    "        y = match.end()\n",
    "        \n",
    "        # On selectionne quelques mots qui entourent le mot-clé \n",
    "        sentence = full_text[max(0, x - 70): y]\n",
    "\n",
    "        # On supprime les références wikipedia eventuellement contenue\n",
    "        sentence = re.sub(r'\\[\\d+\\]', '', sentence)  \n",
    "\n",
    "        print(\"> Phrase séléctionnée: \", sentence.strip())\n",
    "\n",
    "        # On selectionne uniquement le fragment de mots contenus autour de notre mot-clé et \n",
    "        # entre 2 signes de ponctuations si presents\n",
    "        fragments = re.split(r'[.?!;,:\\n]', sentence)\n",
    "        fragment = next((frag.strip() for frag in fragments if word.lower() in frag.lower()), None)\n",
    "        print(\"> Fragment: \",fragment)\n",
    "\n",
    "        # On utilise spaCy pour supprimer les \"stopwords\" \n",
    "        doc = nlp(fragment)\n",
    "        without_stopwords = [token.text for token in doc if not token.is_stop]\n",
    "        sentence = ' '.join(without_stopwords)\n",
    "        print(\"> Fragment sans stopwords:\", sentence)\n",
    "\n",
    "        # On extrait le theme, en utilisant spaCy\n",
    "        for mot in doc:\n",
    "            if mot.text.lower() == word.lower():\n",
    "                mots_lies = []\n",
    "                i = mot.i - 1\n",
    "                while i >= 0:\n",
    "                    prev = doc[i]\n",
    "                    if prev.dep_ in (\"amod\", \"compound\") and prev.pos_ in (\"ADJ\", \"NOUN\"):\n",
    "                        mots_lies.insert(0, prev.text)\n",
    "                        i -= 1\n",
    "                    else:\n",
    "                        break\n",
    "\n",
    "                theme = ' '.join(w.lower() for w in mots_lies + [mot.text])\n",
    "                print(\"> Theme: \", theme)\n",
    "                themes.append(theme)\n",
    "                break  \n",
    "\n",
    "    return themes\n",
    "\n",
    "\n",
    "\n",
    "def wikidata_to_wikipedia(wikidata_url, lang='en'):\n",
    "    entity_id = wikidata_url.strip().split('/')[-1]\n",
    "    api_url = 'https://www.wikidata.org/w/api.php'\n",
    "    params = {'action': 'wbgetentities', 'ids': entity_id, 'format': 'json', 'props': 'sitelinks'}\n",
    "\n",
    "    response = requests.get(api_url, params=params)\n",
    "    data = response.json()\n",
    "\n",
    "    sitelinks = data['entities'].get(entity_id, {}).get('sitelinks', {})\n",
    "    wiki_key = f'{lang}wiki'\n",
    "    if wiki_key in sitelinks:\n",
    "        title = sitelinks[wiki_key].get('title')\n",
    "        if title:\n",
    "            encoded_title = urllib.parse.quote(title.replace(' ', '_'))\n",
    "            return f'https://{lang}.wikipedia.org/wiki/{encoded_title}'\n",
    "    return f\"Non trouvee.\"\n",
    "\n",
    "\n",
    "#theme(\"https://en.wikipedia.org/wiki/Rapha%C3%ABl_Coleman\",\"activist\")\n",
    "#wikidata_to_wikipedia(\"http://www.wikidata.org/entity/Q240573\", lang='en')\n",
    "\n",
    "# G1 = nx.Graph()\n",
    "artistes = json.loads(pathlib.Path(\"artistes.json\").read_text())\n",
    "# with open(\"artistes.json\", \"r\", encoding=\"utf-8\") as f:artistes = json.load(f)\n",
    "# for artiste in artistes:\n",
    "#     nom = artiste.get(\"personLabel\")\n",
    "#     wikidata_url = artiste.get(\"person\")\n",
    "#     wikipedia_url = wikidata_to_wikipedia(wikidata_url)\n",
    "#     if wikipedia_url.startswith(\"http\"):\n",
    "#         themes_extraits = theme(wikipedia_url, \"activist\")\n",
    "#         if themes_extraits:\n",
    "#             G1.add_node(nom, bipartite=0)\n",
    "#             for t in themes_extraits:\n",
    "#                 G1.add_node(t, bipartite=1)\n",
    "#                 G1.add_edge(nom, t)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# left_nodes = {n for n, d in G1.nodes(data=True) if d.get(\"bipartite\") == 0}\n",
    "# right_nodes = {n for n, d in G1.nodes(data=True) if d.get(\"bipartite\") == 1}\n",
    "# pos = nx.bipartite_layout(G1, left_nodes)\n",
    "# plt.figure(figsize=(14, 10))\n",
    "# nx.draw_networkx(\n",
    "#     G1, pos,\n",
    "#     node_color=[\"skyblue\" if n in left_nodes else \"lightgreen\" for n in G1.nodes()],\n",
    "#     node_size=1200,\n",
    "#     font_size=9,\n",
    "#     with_labels=True,\n",
    "#     edge_color=\"gray\"\n",
    "# )\n",
    "# plt.show()\n",
    "\n",
    "# for theme in right_nodes:\n",
    "#     connected_artists = list(G1.neighbors(theme))\n",
    "#     print(f\"Le thème '{theme}' est relié à {len(connected_artists)} artiste(s)\")\n",
    "\n",
    "\n",
    "communautes = {\"climate\" :[\"climate activist\", \"environmental activist\", \"climate change activist\", \"anti - nuclear activist\"],\n",
    "               \"animal rights\":[\"animal rights activist\", \"vegan activist\"],\n",
    "                \"human rights\" : [\"human rights activist\", \"civil rights activist\"],\n",
    "                \"women & lgbt rights\" : [\"feminist activist\", \"gay activist\", \"lesbian activist\", \"transgender activist\", \"sexual purity activist\"],\n",
    "                \"political\" : [\"political activist\"],\n",
    "                \"social\":[\"social activist\", \"social media activist\", \"yellow vest activist\"],\n",
    "                \"health\" : [\"cancer research activist\"]}\n",
    "\n",
    "# G2 = nx.Graph()\n",
    "# artiste_to_themes = {}\n",
    "# for artiste in artistes:\n",
    "#     nom = artiste.get(\"personLabel\")\n",
    "#     wikidata_url = artiste.get(\"person\")\n",
    "#     wikipedia_url = wikidata_to_wikipedia(wikidata_url)\n",
    "#     if wikipedia_url.startswith(\"http\"):\n",
    "#         themes_extraits = theme(wikipedia_url, \"activist\")\n",
    "#         if themes_extraits:\n",
    "#             artiste_ajoute = False\n",
    "#             for t in themes_extraits:\n",
    "#                 for communaute, theme2 in communautes.items():\n",
    "#                     for i in range (len(theme2)):    \n",
    "#                         if theme2[i] in t: \n",
    "#                             if artiste_ajoute == False:\n",
    "#                                 G2.add_node(nom, bipartite=0) \n",
    "#                                 artiste_ajoute = True\n",
    "#                                 G2.add_node(communaute, bipartite=1)\n",
    "#                                 G2.add_edge(nom, communaute)\n",
    "#                                 artiste_to_themes.setdefault(nom, set()).add(communaute)\n",
    "#                                 break\n",
    "#                             else:\n",
    "#                                 G2.add_node(communaute, bipartite=1)\n",
    "#                                 G2.add_edge(nom, communaute)\n",
    "#                                 artiste_to_themes.setdefault(nom, set()).add(communaute)\n",
    "#                                 break\n",
    "\n",
    "\n",
    "# left_nodes = {n for n, d in G2.nodes(data=True) if d.get(\"bipartite\") == 0}\n",
    "# right_nodes = {n for n, d in G2.nodes(data=True) if d.get(\"bipartite\") == 1}\n",
    "# pos = nx.bipartite_layout(G2, left_nodes, align='vertical', scale=5)\n",
    "# plt.figure(figsize=(14, 10))\n",
    "# nx.draw_networkx(\n",
    "#     G2, pos,\n",
    "#     node_color=[\"skyblue\" if n in left_nodes else \"lightgreen\" for n in G2.nodes()],\n",
    "#     node_size=800,\n",
    "#     font_size=7,\n",
    "#     with_labels=True,\n",
    "#     edge_color=\"gray\"\n",
    "# )\n",
    "# plt.show()\n",
    "\n",
    "# with open(\"chanteurs.json\", \"r\", encoding=\"utf-8\") as f:chanteurs = json.load(f)\n",
    "# chanteurs_activists = {}\n",
    "# for personne in artiste_to_themes:\n",
    "#     for chanteur in chanteurs:\n",
    "#         if personne == chanteur.get(\"personLabel\"):\n",
    "#             chanteurs_activists[personne] = list(artiste_to_themes[personne])\n",
    "# print (chanteurs_activists)\n",
    "\n",
    "def related_words(word):\n",
    "    related = set()\n",
    "    for syn in wn.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            related.add(lemma.name())\n",
    "        for hyper in syn.hypernyms():\n",
    "            for lemma in hyper.lemmas():\n",
    "                related.add(lemma.name())\n",
    "        for hypo in syn.hyponyms():\n",
    "            for lemma in hypo.lemmas():\n",
    "                related.add(lemma.name())\n",
    "    return related\n",
    "\n",
    "# On commence maintenant l'analyse des chansons \n",
    "import lyricsgenius\n",
    "genius = lyricsgenius.Genius(\"Fu9F4P_Zft__NzCvberMcG773LOGTEU1bpqA9cdGErZYU41AWnwlmlr649k55FRD\", timeout=15)\n",
    "genius.remove_section_headers = True # Remove section headers (e.g. [Chorus]) from lyrics when searching\n",
    "genius.excluded_terms = [\"(Remix)\", \"(Live)\", \"(Edit)\", \"(Version)\"] # Exclude songs with these words in their title\n",
    "\n",
    "#pour obtenir les paroles d'une chanson\n",
    "def paroles(chanson, chanteur):\n",
    "    song = genius.search_song(chanson, chanteur)\n",
    "    return song.lyrics\n",
    "\n",
    "#pour obtenir les titres des chansons d'un artiste\n",
    "def chansons(chanteur, max_chansons):\n",
    "    artiste = genius.search_artist(chanteur, max_songs=max_chansons, sort=\"popularity\")\n",
    "    return [chanson.title for chanson in artiste.songs]\n",
    "\n",
    "def texte_paroles(texte):\n",
    "    texte = texte.lower()\n",
    "    texte = texte.replace('-', '')\n",
    "    texte = texte.translate(str.maketrans('', '', string.punctuation))\n",
    "    return set(word_tokenize(texte))\n",
    "\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "model_path = \"GoogleNews-vectors-negative300.bin\"\n",
    "model = KeyedVectors.load_word2vec_format(model_path, binary=True,limit=500000)\n",
    "\n",
    "def nettoyer_mot(mot):\n",
    "    mot = mot.replace(\"_\", \" \")          \n",
    "    mot = mot.lower()                     \n",
    "    mot = re.sub(r\"[^\\w\\s]\", \"\", mot)     \n",
    "    return mot.strip()\n",
    "\n",
    "def related_words2(keyword, seuil=0.3):\n",
    "    try:\n",
    "        similar = model.most_similar(keyword, topn=5000000)  \n",
    "        mots = set([word for word, score in similar if score >= seuil])\n",
    "        return set(nettoyer_mot(mot) for mot in mots)\n",
    "    except KeyError:\n",
    "        return set()\n",
    "    \n",
    "\n",
    "#attribue un score de similarité entre les paroles d'une chanson et un theme\n",
    "def score_similarite(chanson, chanteur, theme):\n",
    "    mots_lies = related_words2(theme)\n",
    "    lyrics = paroles(chanson, chanteur)\n",
    "    mots_lyrics = texte_paroles(lyrics)\n",
    "    mots_communs = mots_lyrics.intersection(mots_lies)\n",
    "    print(mots_communs)\n",
    "    #return len(mots_communs) / max(1, len(mots_lyrics))\n",
    "    if len(mots_communs) >= 20:\n",
    "        return \"A+\"\n",
    "    if 15 <= len(mots_communs) <= 20:\n",
    "        return \"A\"\n",
    "    if 10<=len(mots_communs) <= 15:\n",
    "        return \"B\"\n",
    "    if 5<= len(mots_communs) <= 10:\n",
    "        return \"C\"\n",
    "    if 0 < len (mots_communs)<= 5:\n",
    "        return \"D\"\n",
    "    else: \n",
    "        return \"E\"\n",
    "\n",
    "chanteurs_themes = {'M.I.A.': ['politics'], 'KRS-One': ['human rights', 'animal rights'], 'Killer Mike': ['social'], 'Michael Cuccione': ['health'], 'L7a9d': ['human rights'], 'Saba Saba': ['social'], 'Rebecca Moore': ['animal rights'], 'Joaquin Phoenix': ['animal rights'], 'Wyclef Jean': ['politics'], 'Adé Bantu': ['social'], 'Tupac Shakur': ['politics', 'human rights'], 'Mai Khôi': ['politics'], 'Jacline Mouraud': ['social'], 'Madeleina Kay': ['politics'], 'Topher': ['politics'], 'Montana Tucker': ['social']}\n",
    "# for chanteur, themes_associes in chanteurs_themes.items():\n",
    "#      songs = chansons(chanteur, 5)  \n",
    "#      for theme in themes_associes:\n",
    "#          scores = []\n",
    "#          for chanson in songs:\n",
    "#              score_chanson = score_similarite(chanson, chanteur, theme)\n",
    "#              scores.append(score_chanson)\n",
    "#          print (chanteur, theme, scores)\n",
    "\n",
    "\n",
    "mots = [\"pollution\", \"forest\", \"recycling\", \"plastic\", \"energy\", \"biodiversity\", \"climate\", \"oil\",\"deforestation\", \"carbon\", \"agriculture\", \"ocean\", \"sustainable\", \"windmill\", \"compost\",\"glacier\", \"solar\", \"emission\", \"development\", \"hurricane\", \"car\", \"computer\", \"phone\",\"pizza\", \"football\", \"music\", \"school\", \"mountain\", \"internet\", \"coffee\"]\n",
    "\n",
    "def test_seuil(mots, seuil, theme):\n",
    "    try:\n",
    "        similar = model.most_similar(theme, topn=5000000)  \n",
    "        m = set([word for word, score in similar if score >= seuil])\n",
    "        s = set(nettoyer_mot(mot) for mot in m)\n",
    "    except KeyError:\n",
    "        s = set()\n",
    "    mots = set(mots)\n",
    "    mots_communs = mots.intersection(s)\n",
    "    print (mots_communs)\n",
    "    return len (mots_communs)\n",
    "\n",
    "# seuils = np.arange(0, 1.1, 0.1)\n",
    "# longueurs = [test_seuil(mots, seuil, \"environmental\") for seuil in seuils]\n",
    "# plt.plot(seuils, longueurs, marker='o')\n",
    "# plt.title(\"Nombre de mots détectésen fonction du seuil\")\n",
    "# plt.xlabel(\"Seuil de similarité\")\n",
    "# plt.ylabel(\"Nombre de mots en commun\")\n",
    "# plt.grid(True)\n",
    "# plt.show()\n",
    "\n",
    "def nationalite(url):\n",
    "    if not url or not url.startswith(\"http\"):\n",
    "        return \"invalid url\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        return \"error\"\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    infobox = soup.find(\"table\", class_=\"infobox\")\n",
    "    if not infobox:\n",
    "        return \"no infobox found\"\n",
    "    for row in infobox.find_all(\"tr\"):\n",
    "        header = row.find(\"th\")\n",
    "        if header and \"nationality\" in header.get_text(strip=True).lower():\n",
    "            data = row.find(\"td\")\n",
    "            if data:\n",
    "                return data.get_text(strip=True)\n",
    "    for row in infobox.find_all(\"tr\"):\n",
    "        header = row.find(\"th\")\n",
    "        if header and \"born\" in header.get_text(strip=True).lower():\n",
    "            data = row.find(\"td\")\n",
    "            if data:\n",
    "                lines = data.get_text(separator=\"\\n\", strip=True).split(\"\\n\")\n",
    "                if lines:\n",
    "                    last_line = lines[-1].strip()\n",
    "                    if ',' in last_line:\n",
    "                        parts = [p.strip() for p in last_line.split(',')]\n",
    "                        return parts[-1]\n",
    "                    return last_line\n",
    "    return \"not found\"\n",
    "\n",
    "with open(\"chanteurs.json\", \"r\", encoding=\"utf-8\") as f:chanteurs = json.load(f)\n",
    "noms_filtres = set(chanteurs_themes.keys())\n",
    "\n",
    "# chanteurs_nationalite = {}\n",
    "\n",
    "# for chanteur in chanteurs:\n",
    "#     nom = chanteur.get(\"personLabel\")\n",
    "#     if nom in noms_filtres:\n",
    "#         wikidata_url = chanteur.get(\"person\")\n",
    "#         wikipedia_url = wikidata_to_wikipedia(wikidata_url)\n",
    "#         if not wikipedia_url or not wikipedia_url.startswith(\"http\"):\n",
    "#             pays = \"invalid url\"\n",
    "#         else:\n",
    "#             pays = nationalite(wikipedia_url)\n",
    "#         chanteurs_nationalite[nom] = pays\n",
    "# print(chanteurs_nationalite)\n",
    "\n",
    "def genres(url):\n",
    "    if not url or not url.startswith(\"http\"):\n",
    "        return \"invalid url\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        return \"error\"\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    infobox = soup.find(\"table\", class_=\"infobox\")\n",
    "    if not infobox:\n",
    "        return \"no infobox found\"\n",
    "    for row in infobox.find_all(\"tr\"):\n",
    "        header = row.find(\"th\")\n",
    "        if header and \"genres\" in header.get_text(strip=True).lower():\n",
    "            data = row.find(\"td\")\n",
    "            if data:\n",
    "                return data.get_text(separator=\", \", strip=True)\n",
    "    return \"not found\"\n",
    "\n",
    "# chanteurs_genres = {}\n",
    "\n",
    "# for chanteur in chanteurs:\n",
    "#     nom = chanteur.get(\"personLabel\")\n",
    "#     if nom in noms_filtres:\n",
    "#         wikidata_url = chanteur.get(\"person\")\n",
    "#         wikipedia_url = wikidata_to_wikipedia(wikidata_url)\n",
    "#         if not wikipedia_url or not wikipedia_url.startswith(\"http\"):\n",
    "#             genre = \"invalid url\"\n",
    "#         else:\n",
    "#             genre = genres(wikipedia_url)\n",
    "#         chanteurs_genres[nom] = genre\n",
    "\n",
    "# chanteurs_genres2 = {}\n",
    "\n",
    "# # Nettoyer la liste obtenue et ne laisser que les artistes qui possèdent des genres\n",
    "# for artist, genres in chanteurs_genres.items():\n",
    "#     if genres == \"not found\" or not genres:\n",
    "#         continue  \n",
    "#     cleaned = re.sub(r\"[\\[\\]0-9/]\", \"\", genres)\n",
    "#     cleaned = re.sub(r\",\\s*,+\", \",\", cleaned)  \n",
    "#     cleaned = re.sub(r\"\\s+\", \" \", cleaned) \n",
    "#     cleaned = [g.strip().lower() for g in cleaned.split(\",\") if g.strip()]\n",
    "#     chanteurs_genres2[artist] = list(set(cleaned)) \n",
    "# print (chanteurs_genres2)\n",
    "\n",
    "with open(\"femmes.json\", \"r\", encoding=\"utf-8\") as f:femmes = json.load(f)\n",
    "\n",
    "# chanteurs_gender = {}\n",
    "\n",
    "# for chanteur in chanteurs:\n",
    "#     nom = chanteur.get(\"personLabel\")\n",
    "#     if nom in noms_filtres:\n",
    "#         if chanteur in femmes:\n",
    "#             chanteurs_gender[nom] = \"femme\"\n",
    "#         else:\n",
    "#             chanteurs_gender[nom] = \"homme\"\n",
    "\n",
    "def naissance(url):\n",
    "    if not url or not url.startswith(\"http\"):\n",
    "        return \"invalid url\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        return \"error\"\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    infobox = soup.find(\"table\", class_=\"infobox\")\n",
    "    if not infobox:\n",
    "        return \"no infobox found\"\n",
    "    rows = infobox.find_all(\"tr\")\n",
    "    for row in rows:\n",
    "        header = row.find(\"th\")\n",
    "        if header and \"born\" in header.text.lower():\n",
    "            born_cell = row.find(\"td\")\n",
    "            if born_cell:\n",
    "                text = born_cell.get_text(\" \", strip=True)\n",
    "                match = re.search(r\"\\b(19[6-9][0-9]|20[0-9]{2})\\b\", text)\n",
    "                if match:\n",
    "                    return int(match.group(1))\n",
    "                else:\n",
    "                    return \"no valid year found\"\n",
    "    return \"no 'born' field found\"\n",
    "    \n",
    "# chanteurs_naissance = {}\n",
    "# for chanteur in chanteurs:\n",
    "#     nom = chanteur.get(\"personLabel\")\n",
    "#     if nom in noms_filtres:\n",
    "#         wikidata_url = chanteur.get(\"person\")\n",
    "#         wikipedia_url = wikidata_to_wikipedia(wikidata_url)\n",
    "#         if not wikipedia_url or not wikipedia_url.startswith(\"http\"):\n",
    "#             annee = \"invalid url\"\n",
    "#         else:\n",
    "#             annee = naissance(wikipedia_url)\n",
    "#         chanteurs_naissance[nom] = annee\n",
    "# print (chanteurs_naissance)\n",
    "\n",
    "chanteurs_gender2 = {'KRS-One': 'homme', 'Tupac Shakur': 'homme', 'Adé Bantu': 'homme', 'Joaquin Phoenix': 'homme', 'Wyclef Jean': 'homme', 'Killer Mike': 'homme', 'M.I.A.': 'femme', 'Saba Saba': 'homme', 'Mai Khôi': 'femme', 'Michael Cuccione': 'homme', 'L7a9d': 'homme', 'Rebecca Moore': 'femme', 'Madeleina Kay': 'femme', 'Jacline Mouraud': 'femme', 'Topher': 'homme', 'Montana Tucker': 'femme'}    \n",
    "chanteurs_nationalite2 =  {'KRS-One': 'U.S.', 'Tupac Shakur': 'U.S.', 'Adé Bantu': 'England', 'Joaquin Phoenix': 'Puerto Rico', 'Wyclef Jean': 'Haiti', 'Killer Mike': 'U.S.', 'M.I.A.': 'England', 'Saba Saba': 'Uganda', 'Mai Khôi': 'Vietnam', 'Michael Cuccione': 'Canada', 'L7a9d': 'Morocco', 'Rebecca Moore': 'U.S.', 'Madeleina Kay': 'England', 'Jacline Mouraud': 'France', 'Topher': 'U.S.', 'Montana Tucker': 'U.S.'}\n",
    "chanteurs_genres3 = {'KRS-One': ['conscious rap', 'hardcore hip-hop', 'political hip-hop', 'east coast hip-hop'], 'Tupac Shakur': ['political hip-hop', 'west coast hip-hop', 'gangsta rap'], 'Adé Bantu': ['hip-hop', 'afrofunk', 'fuji', 'afrobeat'], 'Wyclef Jean': ['east coast hip hop', 'neo soul', 'r&b', 'reggae fusion', 'pop rap'], 'Killer Mike': ['southern hip-hop', 'gospel'], 'M.I.A.': ['hip hop', 'alternative hip hop', 'world', 'dance', 'progressive rap', 'pop', 'electronica', 'experimental'], 'Saba Saba': ['hip hop', 'african hip hop'], 'Mai Khôi': ['pop'], 'L7a9d': ['rap'], 'Rebecca Moore': ['alternative', 'ambient', 'experimental'], 'Topher': ['hip hop']}\n",
    "chanteurs_naissance2 = {'KRS-One': 1965, 'Tupac Shakur': 1971, 'Adé Bantu': 1971, 'Joaquin Phoenix': 1974, 'Wyclef Jean': 1969, 'Killer Mike': 1975, 'M.I.A.': 1975, 'Saba Saba': 1977, 'Mai Khôi': 1983, 'Michael Cuccione': 1985, 'L7a9d': 1988, 'Rebecca Moore': 1968, 'Madeleina Kay': 1994, 'Jacline Mouraud': 1967, 'Topher': 1991, 'Montana Tucker': 1993}\n",
    "\n",
    "# chanteurs = chanteurs_gender2.keys()\n",
    "# print (chanteurs)\n",
    "# dictionnaire = [\n",
    "#     {'nationalite': chanteurs_nationalite2.get(chanteur), 'gender': chanteurs_gender2.get(chanteur), 'genres':chanteurs_genres3.get(chanteur, []), 'naissance':chanteurs_naissance2.get(chanteur)/1965}\n",
    "#     for chanteur in chanteurs\n",
    "# ]\n",
    "# print (dictionnaire)\n",
    "\n",
    "# vec= DictVectorizer()\n",
    "# X = vec.fit_transform(dictionnaire).toarray()\n",
    "\n",
    "# for k in range(2,len(X[0])):\n",
    "#     print(\"number of clusters: \"+ str(k))\n",
    "#     kmeans = KMeans(n_clusters=k, random_state=0).fit(X)\n",
    "#     print(silhouette_score(X, kmeans.labels_))\n",
    "#     print(kmeans.labels_)\n",
    "\n",
    "america = ['U.S.','Canada','Haiti', 'Puerto Rico', 'Brazil']\n",
    "europe = ['England', 'France']\n",
    "africa = ['Uganda','Morocco','Nigeria', 'South Africa']\n",
    "asia = ['India', 'Afghanistan', 'Hong Kong', 'Vietnam', 'Pakistan', 'Bangladesh']\n",
    "\n",
    "\n",
    "def continent(pays):\n",
    "    if pays in america:\n",
    "        return \"america\"\n",
    "    elif pays in europe:\n",
    "        return \"europe\"\n",
    "    elif pays in africa:\n",
    "        return \"africa\"\n",
    "    elif pays in asia:\n",
    "        return \"asia\"\n",
    "    return None\n",
    "\n",
    "\n",
    "# G3 = nx.Graph()\n",
    "\n",
    "# for chanteur in chanteurs_gender2:\n",
    "#     G3.add_node(chanteur)\n",
    "# for a, b in combinations(chanteurs_gender2, 2):\n",
    "#     weight = 0\n",
    "#     if chanteurs_gender2[a] == chanteurs_gender2[b]:\n",
    "#         weight += 1\n",
    "#     nat_a = chanteurs_nationalite2.get(a)\n",
    "#     nat_b = chanteurs_nationalite2.get(b)\n",
    "#     if nat_a == nat_b:\n",
    "#         weight += 2\n",
    "#     elif continent(nat_a) == continent(nat_b):\n",
    "#         weight += 1\n",
    "#     genres_a = set(chanteurs_genres3.get(a, []))\n",
    "#     genres_b = set(chanteurs_genres3.get(b, []))\n",
    "#     weight += len(genres_a & genres_b)  \n",
    "#     y_a = chanteurs_naissance2.get(a)\n",
    "#     y_b = chanteurs_naissance2.get(b)\n",
    "#     if y_a == y_b:\n",
    "#         weight += 2\n",
    "#     elif abs(y_a - y_b) <= 5:\n",
    "#         weight += 1\n",
    "#     if weight > 0:\n",
    "#         G3.add_edge(a, b, weight=weight)\n",
    "\n",
    "# pos = nx.spring_layout(G3, seed=42)\n",
    "# edges = G3.edges(data=True)\n",
    "# weights = [d['weight'] for (_, _, d) in edges]\n",
    "# plt.figure(figsize=(13, 11))\n",
    "# nx.draw_networkx_nodes(G3, pos, node_color='lightblue', node_size=1000)\n",
    "# nx.draw_networkx_labels(G3, pos, font_size=10)\n",
    "# nx.draw_networkx_edges(G3, pos, width=weights, edge_color='gray')\n",
    "# plt.axis('off')\n",
    "# plt.show()\n",
    "\n",
    "# louvain=nx.community.louvain_communities(G3, weight='weight')\n",
    "# print(\"Louvain Communities:\", louvain)\n",
    "\n",
    "# louvain_dict = {}\n",
    "# for i, community in enumerate(louvain):\n",
    "#     for node in community:\n",
    "#         louvain_dict[node] = i\n",
    "\n",
    "# pos = nx.spring_layout(G3)\n",
    "# cmap = plt.cm.get_cmap('tab10')\n",
    "# nx.draw_networkx_nodes(G3, pos, node_color=[louvain_dict[n] for n in G3.nodes()], cmap=cmap)\n",
    "# nx.draw_networkx_edges(G3, pos, width=[G3[u][v]['weight'] for u, v in G3.edges()])\n",
    "# nx.draw_networkx_labels(G3, pos)\n",
    "# plt.show()\n",
    "\n",
    "# communautes = [{'Michael Cuccione', 'KRS-One', 'Topher', 'L7a9d', 'Tupac Shakur', 'Saba Saba', 'Killer Mike', 'Wyclef Jean', 'Joaquin Phoenix'},{'Rebecca Moore', 'Montana Tucker', 'Madeleina Kay', 'Adé Bantu', 'M.I.A.', 'Jacline Mouraud', 'Mai Khôi'}]\n",
    "# themes_par_communaute = []\n",
    "\n",
    "# for i, comm in enumerate(communautes):\n",
    "#     themes = []\n",
    "#     for chanteur in comm:\n",
    "#         if chanteur in chanteurs_themes:\n",
    "#             themes.extend(chanteurs_themes[chanteur])\n",
    "#     themes_uniques = list(set(themes))\n",
    "#     themes_par_communaute.append(themes_uniques)\n",
    "\n",
    "# for i, themes in enumerate(themes_par_communaute):\n",
    "#     print(f\"Communauté {i+1} : {themes}\")\n",
    "\n",
    "def analyse_communaute(communaute):\n",
    "    genres = []\n",
    "    continents = []\n",
    "    ages = []\n",
    "    genres_musicaux = []\n",
    "    for chanteur in communaute:\n",
    "        genres.append(chanteurs_gender2.get(chanteur))\n",
    "        nat = chanteurs_nationalite2.get(chanteur)\n",
    "        continents.append(continent(nat))\n",
    "        naissance = chanteurs_naissance2.get(chanteur)\n",
    "        if naissance:\n",
    "            ages.append(naissance)\n",
    "        genres_musicaux.extend(chanteurs_genres3.get(chanteur, []))\n",
    "    return {\n",
    "        'genre': Counter(genres),\n",
    "        'continent': Counter(continents),\n",
    "        'annee_naissance_moyenne': round(np.mean(ages), 1) if ages else None,\n",
    "        'genres_musicaux_frequents': Counter(genres_musicaux).most_common(5)\n",
    "    }\n",
    "# res_1 = analyse_communaute(communautes[0])\n",
    "# res_2 = analyse_communaute(communautes[1])\n",
    "# print(\"Communauté bleu foncé :\", res_1)\n",
    "# print(\"Communauté bleu clair :\", res_2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# on fait le travail pour les artistes et pas uniquement les chanteurs\n",
    "artiste_to_themes = {'M.I.A.': {'political'}, 'Arundhati Roy': {'political'}, 'Dexter Scott King': {'animal rights', 'human rights'}, 'Alexandra Paul': {'animal rights'}, 'Talib Kweli': {'political'}, 'KRS-One': {'animal rights', 'human rights'}, 'Killer Mike': {'social'}, 'Michael Cuccione': {'health'}, 'L7a9d': {'human rights'}, 'Anneka Svenska': {'animal rights'}, 'Sonia Nassery Cole': {'human rights'}, 'Saba Saba': {'social'}, 'Tam Tak-chi': {'social'}, 'Rebecca Moore': {'animal rights'}, 'Tembi Locke': {'human rights'}, 'Joaquin Phoenix': {'animal rights'}, 'Raphaël Coleman': {'climate'}, 'Ben Patrick Johnson': {'women & lgbt rights', 'human rights'}, 'Cyrus Grace Dunham': {'women & lgbt rights'}, 'Wyclef Jean': {'political'}, 'Adé Bantu': {'social'}, 'Luisa Mell': {'animal rights'}, 'Emma Watson': {'climate'}, 'Tupac Shakur': {'political', 'human rights'}, 'Lesego Motsepe': {'social'}, 'Dallas Goldtooth': {'climate'}, 'Mai Khôi': {'political'}, 'Jacline Mouraud': {'social'}, 'Cat Brooks': {'climate'}, 'Madeleina Kay': {'political'}, 'Ronen Rubinstein': {'climate'}, 'Qandeel Baloch': {'women & lgbt rights'}, 'Shitou': {'women & lgbt rights'}, 'Malynda Hale': {'political'}, 'Juliana Olayode': {'women & lgbt rights'}, 'Topher': {'political'}, 'Samina Luthfa': {'social'}, 'Montana Tucker': {'social'}}\n",
    "artistes_filtres = set(artiste_to_themes.keys())\n",
    "\n",
    "# artistes_nationalite = {}\n",
    "\n",
    "# for artiste in artistes:\n",
    "#     nom = artiste.get(\"personLabel\")\n",
    "#     if nom in artistes_filtres:\n",
    "#         wikidata_url = artiste.get(\"person\")\n",
    "#         wikipedia_url = wikidata_to_wikipedia(wikidata_url)\n",
    "#         if not wikipedia_url or not wikipedia_url.startswith(\"http\"):\n",
    "#             pays = \"invalid url\"\n",
    "#         else:\n",
    "#             pays = nationalite(wikipedia_url)\n",
    "#         artistes_nationalite[nom] = pays\n",
    "# print(artistes_nationalite)\n",
    "\n",
    "# artistes_naissance = {}\n",
    "# for artiste in artistes:\n",
    "#     nom = artiste.get(\"personLabel\")\n",
    "#     if nom in artistes_filtres:\n",
    "#         wikidata_url = artiste.get(\"person\")\n",
    "#         wikipedia_url = wikidata_to_wikipedia(wikidata_url)\n",
    "#         if not wikipedia_url or not wikipedia_url.startswith(\"http\"):\n",
    "#             annee = \"invalid url\"\n",
    "#         else:\n",
    "#             annee = naissance(wikipedia_url)\n",
    "#         artistes_naissance[nom] = annee\n",
    "# print (artistes_naissance)\n",
    "\n",
    "# with open(\"femmes2.json\", \"r\", encoding=\"utf-8\") as f:femmes2 = json.load(f)\n",
    "\n",
    "# artistes_gender = {}\n",
    "# for artiste in artistes:\n",
    "#     nom = artiste.get(\"personLabel\")\n",
    "#     if nom in artistes_filtres:\n",
    "#         if artiste in femmes2:\n",
    "#             artistes_gender[nom] = \"femme\"\n",
    "#         else:\n",
    "#             artistes_gender[nom] = \"homme\"\n",
    "# print (artistes_gender)\n",
    "\n",
    "\n",
    "artistes_nationalite2 = {'M.I.A.': 'England', 'Arundhati Roy': 'India', 'Dexter Scott King': 'U.S.', 'Alexandra Paul': 'U.S.', 'Talib Kweli': 'U.S.', 'KRS-One': 'U.S.', 'Killer Mike': 'U.S.', 'Michael Cuccione': 'Canada', 'L7a9d': 'Morocco', 'Anneka Svenska': 'England', 'Sonia Nassery Cole': 'Afghanistan', 'Saba Saba': 'Uganda', 'Tam Tak-chi': 'Hong Kong', 'Rebecca Moore': 'U.S.',  'Joaquin Phoenix': 'Puerto Rico', 'Raphaël Coleman': 'England', 'Ben Patrick Johnson': 'U.S.', 'Cyrus Grace Dunham': 'U.S.', 'Wyclef Jean': 'Haiti', 'Adé Bantu': 'England', 'Luisa Mell': 'Brazil', 'Emma Watson': 'France', 'Tupac Shakur': 'U.S.', 'Lesego Motsepe': 'South Africa', 'Dallas Goldtooth': 'U.S.', 'Mai Khôi': 'Vietnam', 'Jacline Mouraud': 'France', 'Cat Brooks': 'U.S.', 'Madeleina Kay': 'England', 'Qandeel Baloch': 'Pakistan', 'Malynda Hale': 'U.S.', 'Juliana Olayode': 'Nigeria', 'Topher': 'U.S.', 'Samina Luthfa': 'Bangladesh', 'Montana Tucker': 'U.S.'}\n",
    "artistes_naissance2 = {'M.I.A.': 1975, 'Arundhati Roy': 1961, 'Dexter Scott King': 1961, 'Alexandra Paul': 1963, 'Talib Kweli': 1975, 'KRS-One': 1965, 'Killer Mike': 1975, 'Michael Cuccione': 1985, 'L7a9d': 1988, 'Anneka Svenska': 1974, 'Sonia Nassery Cole': 1965, 'Saba Saba': 1977, 'Tam Tak-chi': 1973, 'Rebecca Moore': 1968,  'Joaquin Phoenix': 1974, 'Raphaël Coleman': 1994, 'Ben Patrick Johnson': 1969, 'Cyrus Grace Dunham': 1992, 'Wyclef Jean': 1969, 'Adé Bantu': 1971, 'Luisa Mell': 1978, 'Emma Watson': 1990, 'Tupac Shakur': 1971, 'Lesego Motsepe': 1974, 'Dallas Goldtooth': 1983, 'Mai Khôi': 1983, 'Jacline Mouraud': 1967, 'Cat Brooks': 1975, 'Madeleina Kay': 1994, 'Qandeel Baloch': 1990,  'Malynda Hale': 1986, 'Juliana Olayode': 1995, 'Topher': 1991, 'Montana Tucker': 1993}\n",
    "artistes_gender2 = {'M.I.A.': 'femme', 'Arundhati Roy': 'femme', 'Dexter Scott King': 'homme', 'Alexandra Paul': 'femme', 'Talib Kweli': 'homme', 'KRS-One': 'homme', 'Killer Mike': 'homme', 'Michael Cuccione': 'homme', 'L7a9d': 'homme', 'Anneka Svenska': 'femme', 'Sonia Nassery Cole': 'femme', 'Saba Saba': 'homme', 'Tam Tak-chi': 'homme', 'Rebecca Moore': 'femme', 'Tembi Locke': 'femme', 'Joaquin Phoenix': 'homme', 'Raphaël Coleman': 'homme', 'Ben Patrick Johnson': 'homme', 'Cyrus Grace Dunham': 'homme', 'Wyclef Jean': 'homme', 'Adé Bantu': 'homme', 'Luisa Mell': 'femme', 'Emma Watson': 'femme', 'Tupac Shakur': 'homme', 'Lesego Motsepe': 'femme', 'Dallas Goldtooth': 'homme', 'Mai Khôi': 'femme', 'Jacline Mouraud': 'femme', 'Cat Brooks': 'femme', 'Madeleina Kay': 'femme', 'Qandeel Baloch': 'femme', 'Shitou': 'femme', 'Malynda Hale': 'femme', 'Juliana Olayode': 'femme', 'Topher': 'homme', 'Samina Luthfa': 'femme', 'Montana Tucker': 'femme'}\n",
    "artistes_communs = set(artistes_nationalite2.keys()) & set(artistes_naissance2.keys()) & set(artistes_gender2.keys())\n",
    "\n",
    "# artistes = artistes_gender2.keys()\n",
    "# print (artistes)\n",
    "# dictionnaire = [\n",
    "#     {'nationalite': artistes_nationalite2.get(artiste), 'gender': artistes_gender2.get(artiste), 'naissance':artistes_naissance2.get(artiste)/1961}\n",
    "#     for artiste in artistes_communs\n",
    "# ]\n",
    "# print (dictionnaire)\n",
    "\n",
    "# vec= DictVectorizer()\n",
    "# X = vec.fit_transform(dictionnaire).toarray()\n",
    "\n",
    "# for k in range(2,len(X)):\n",
    "#     print(\"number of clusters: \"+ str(k))\n",
    "#     kmeans = KMeans(n_clusters=k, random_state=0).fit(X)\n",
    "#     print(silhouette_score(X, kmeans.labels_))\n",
    "#     print(kmeans.labels_)\n",
    "\n",
    "# G4 = nx.Graph()\n",
    "\n",
    "# for artiste in artistes_communs:\n",
    "#     G4.add_node(artiste)\n",
    "# for a, b in combinations(artistes_communs, 2):\n",
    "#     weight = 0\n",
    "#     if artistes_gender2[a] == artistes_gender2[b]:\n",
    "#         weight += 1\n",
    "#     nat_a = artistes_nationalite2.get(a)\n",
    "#     nat_b = artistes_nationalite2.get(b)\n",
    "#     if nat_a == nat_b:\n",
    "#         weight += 2\n",
    "#     elif continent(nat_a) == continent(nat_b):\n",
    "#         weight += 1\n",
    "#     y_a = artistes_naissance2.get(a)\n",
    "#     y_b = artistes_naissance2.get(b)\n",
    "#     if y_a == y_b:\n",
    "#         weight += 2\n",
    "#     elif abs(y_a - y_b) <= 5:\n",
    "#         weight += 1\n",
    "#     if weight > 0:\n",
    "#         G4.add_edge(a, b, weight=weight)\n",
    "\n",
    "# pos = nx.spring_layout(G4, seed=42)\n",
    "# edges = G4.edges(data=True)\n",
    "# weights = [d['weight'] for (_, _, d) in edges]\n",
    "# plt.figure(figsize=(13, 11))\n",
    "# nx.draw_networkx_nodes(G4, pos, node_color='lightblue', node_size=1000)\n",
    "# nx.draw_networkx_labels(G4, pos, font_size=10)\n",
    "# nx.draw_networkx_edges(G4, pos, width=weights, edge_color='gray')\n",
    "# plt.axis('off')\n",
    "# plt.show()\n",
    "\n",
    "# louvain=nx.community.louvain_communities(G4, weight='weight')\n",
    "# print(\"Louvain Communities:\", louvain)\n",
    "\n",
    "# louvain_dict = {}\n",
    "# for i, community in enumerate(louvain):\n",
    "#     for node in community:\n",
    "#         louvain_dict[node] = i\n",
    "\n",
    "# pos = nx.spring_layout(G4)\n",
    "# cmap = plt.cm.get_cmap('tab10')\n",
    "# nx.draw_networkx_nodes(G4, pos, node_color=[louvain_dict[n] for n in G4.nodes()], cmap=cmap)\n",
    "# nx.draw_networkx_edges(G4, pos, width=0.45)\n",
    "# nx.draw_networkx_labels(G4, pos)\n",
    "# plt.show()\n",
    "\n",
    "communautes = [{'Cyrus Grace Dunham', 'KRS-One', 'Topher', 'Rebecca Moore', 'Michael Cuccione', 'Tupac Shakur', 'Cat Brooks', 'Ben Patrick Johnson', 'Talib Kweli', 'Dallas Goldtooth', 'Alexandra Paul', 'Dexter Scott King', 'Joaquin Phoenix', 'Montana Tucker', 'Wyclef Jean', 'Malynda Hale', 'Killer Mike'}, {'L7a9d', 'Saba Saba', 'Tam Tak-chi'}, {'Luisa Mell', 'Raphaël Coleman', 'Sonia Nassery Cole', 'Qandeel Baloch', 'Madeleina Kay', 'Arundhati Roy', 'M.I.A.', 'Juliana Olayode', 'Emma Watson', 'Adé Bantu', 'Lesego Motsepe', 'Mai Khôi', 'Anneka Svenska', 'Jacline Mouraud'}]\n",
    "themes_par_communaute = []\n",
    "\n",
    "for i, comm in enumerate(communautes):\n",
    "    themes = []\n",
    "    for artiste in comm:\n",
    "        if artiste in artiste_to_themes:\n",
    "            themes.extend(artiste_to_themes[artiste])\n",
    "    themes_uniques = list(set(themes))\n",
    "    themes_par_communaute.append(themes_uniques)\n",
    "\n",
    "for i, themes in enumerate(themes_par_communaute):\n",
    "    print(f\"Communauté {i+1} : {themes}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['KRS-One', 'Tupac Shakur', 'Adé Bantu', 'Joaquin Phoenix', 'Wyclef Jean', 'Killer Mike', 'M.I.A.', 'Saba Saba', 'Mai Khôi', 'Michael Cuccione', 'L7a9d', 'Rebecca Moore', 'Madeleina Kay', 'Jacline Mouraud', 'Topher', 'Montana Tucker'])\n",
      "[{'nationalite': 'U.S.', 'gender': 'homme', 'genres': ['conscious rap', 'hardcore hip-hop', 'political hip-hop', 'east coast hip-hop'], 'naissance': 1.0}, {'nationalite': 'U.S.', 'gender': 'homme', 'genres': ['political hip-hop', 'west coast hip-hop', 'gangsta rap'], 'naissance': 1.0030534351145037}, {'nationalite': 'England', 'gender': 'homme', 'genres': ['hip-hop', 'afrofunk', 'fuji', 'afrobeat'], 'naissance': 1.0030534351145037}, {'nationalite': 'Puerto Rico', 'gender': 'homme', 'genres': [], 'naissance': 1.0045801526717557}, {'nationalite': 'Haiti', 'gender': 'homme', 'genres': ['east coast hip hop', 'neo soul', 'r&b', 'reggae fusion', 'pop rap'], 'naissance': 1.0020356234096692}, {'nationalite': 'U.S.', 'gender': 'homme', 'genres': ['southern hip-hop', 'gospel'], 'naissance': 1.005089058524173}, {'nationalite': 'England', 'gender': 'femme', 'genres': ['hip hop', 'alternative hip hop', 'world', 'dance', 'progressive rap', 'pop', 'electronica', 'experimental'], 'naissance': 1.005089058524173}, {'nationalite': 'Uganda', 'gender': 'homme', 'genres': ['hip hop', 'african hip hop'], 'naissance': 1.0061068702290077}, {'nationalite': 'Vietnam', 'gender': 'femme', 'genres': ['pop'], 'naissance': 1.0091603053435114}, {'nationalite': 'Canada', 'gender': 'homme', 'genres': [], 'naissance': 1.0101781170483461}, {'nationalite': 'Morocco', 'gender': 'homme', 'genres': ['rap'], 'naissance': 1.011704834605598}, {'nationalite': 'U.S.', 'gender': 'femme', 'genres': ['alternative', 'ambient', 'experimental'], 'naissance': 1.001526717557252}, {'nationalite': 'England', 'gender': 'femme', 'genres': [], 'naissance': 1.0147582697201019}, {'nationalite': 'France', 'gender': 'femme', 'genres': [], 'naissance': 1.0010178117048345}, {'nationalite': 'U.S.', 'gender': 'homme', 'genres': ['hip hop'], 'naissance': 1.0132315521628499}, {'nationalite': 'U.S.', 'gender': 'femme', 'genres': [], 'naissance': 1.0142493638676844}]\n",
      "number of clusters: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/threadpoolctl.py:1214: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14386220038723613\n",
      "[1 1 1 1 1 1 0 1 0 1 1 0 0 0 1 0]\n",
      "number of clusters: 3\n",
      "0.11124885856181935\n",
      "[2 2 1 1 1 2 0 1 0 1 1 0 0 0 1 0]\n",
      "number of clusters: 4\n",
      "0.09201634650459678\n",
      "[2 2 1 1 1 2 0 1 0 1 1 3 3 3 1 3]\n",
      "number of clusters: 5\n",
      "0.06164298999368524\n",
      "[2 2 1 1 1 2 4 1 0 1 1 3 3 3 1 3]\n",
      "number of clusters: 6\n",
      "0.03983702428772151\n",
      "[5 2 1 1 1 2 4 1 0 1 1 3 3 3 1 3]\n",
      "number of clusters: 7\n",
      "0.019427579947971195\n",
      "[5 6 1 1 1 2 4 1 0 1 1 3 3 3 1 3]\n",
      "number of clusters: 8\n",
      "0.052034709036519876\n",
      "[4 5 7 1 6 5 3 5 0 1 1 2 2 2 5 2]\n",
      "number of clusters: 9\n",
      "0.06589590380116987\n",
      "[4 5 7 1 6 5 3 5 0 1 1 8 2 2 5 2]\n",
      "number of clusters: 10\n",
      "0.07115003152449487\n",
      "[4 9 7 1 6 5 3 5 0 1 1 8 2 2 5 2]\n",
      "number of clusters: 11\n",
      "0.07583469931876811\n",
      "[ 4  9  7  1  6  5  3 10  0  1  1  8  2  2  5  2]\n",
      "number of clusters: 12\n",
      "0.06571352615233855\n",
      "[ 4  9  7  1  6  5  3 10  0  1 11  8  2  2  5  2]\n",
      "number of clusters: 13\n",
      "0.057340664269886946\n",
      "[ 4  9  7  1  6 12  3 10  0  1 11  8  2  2  5  2]\n",
      "number of clusters: 14\n",
      "0.022943256408860968\n",
      "[ 4  9  7  1  6 12  3 10  0  1 11  8  2 13  5  2]\n",
      "number of clusters: 15\n",
      "5.677052391988005e-06\n",
      "[ 4  9  7 14  6 12  3 10  0  1 11  8  2 13  5  2]\n",
      "number of clusters: 16\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Number of labels is 16. Valid values are 2 to n_samples - 1 (inclusive)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 485\u001b[0m\n\u001b[1;32m    483\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumber of clusters: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(k))\n\u001b[1;32m    484\u001b[0m kmeans \u001b[38;5;241m=\u001b[39m KMeans(n_clusters\u001b[38;5;241m=\u001b[39mk, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[0;32m--> 485\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43msilhouette_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkmeans\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabels_\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    486\u001b[0m \u001b[38;5;28mprint\u001b[39m(kmeans\u001b[38;5;241m.\u001b[39mlabels_)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:216\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    212\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    213\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    214\u001b[0m         )\n\u001b[1;32m    215\u001b[0m     ):\n\u001b[0;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    224\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    226\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/metrics/cluster/_unsupervised.py:139\u001b[0m, in \u001b[0;36msilhouette_score\u001b[0;34m(X, labels, metric, sample_size, random_state, **kwds)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    138\u001b[0m         X, labels \u001b[38;5;241m=\u001b[39m X[indices], labels[indices]\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmean(\u001b[43msilhouette_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:189\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m global_skip_validation \u001b[38;5;241m=\u001b[39m get_config()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip_parameter_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n\u001b[0;32m--> 189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m func_sig \u001b[38;5;241m=\u001b[39m signature(func)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/metrics/cluster/_unsupervised.py:297\u001b[0m, in \u001b[0;36msilhouette_samples\u001b[0;34m(X, labels, metric, **kwds)\u001b[0m\n\u001b[1;32m    295\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(labels)\n\u001b[1;32m    296\u001b[0m label_freqs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mbincount(labels)\n\u001b[0;32m--> 297\u001b[0m \u001b[43mcheck_number_of_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclasses_\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    299\u001b[0m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetric\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m metric\n\u001b[1;32m    300\u001b[0m reduce_func \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(\n\u001b[1;32m    301\u001b[0m     _silhouette_reduce, labels\u001b[38;5;241m=\u001b[39mlabels, label_freqs\u001b[38;5;241m=\u001b[39mlabel_freqs\n\u001b[1;32m    302\u001b[0m )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/metrics/cluster/_unsupervised.py:36\u001b[0m, in \u001b[0;36mcheck_number_of_labels\u001b[0;34m(n_labels, n_samples)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check that number of labels are valid.\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \n\u001b[1;32m     27\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;124;03m    Number of samples.\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m<\u001b[39m n_labels \u001b[38;5;241m<\u001b[39m n_samples:\n\u001b[0;32m---> 36\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     37\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of labels is \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. Valid values are 2 to n_samples - 1 (inclusive)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     38\u001b[0m         \u001b[38;5;241m%\u001b[39m n_labels\n\u001b[1;32m     39\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Number of labels is 16. Valid values are 2 to n_samples - 1 (inclusive)"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import urllib.parse\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import spacy\n",
    "import json\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from networkx.algorithms import bipartite\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import pathlib\n",
    "import sklearn\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "    \n",
    "def occurences(url, word):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        return (\"erreur\")\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    paragraphs = soup.find_all('p')\n",
    "    full_text = ' '.join(p.get_text() for p in paragraphs)\n",
    "\n",
    "    occurences = 0\n",
    "    for match in re.finditer(rf'\\b{re.escape(word)}\\b', full_text, re.IGNORECASE):\n",
    "        occurences += 1\n",
    "        print(f\"{occurences} starts at: {match.start()} ends at: {match.end()}\")\n",
    "\n",
    "    return occurences\n",
    "\n",
    "\n",
    "\n",
    "def theme(url, word):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    paragraphs = soup.find_all('p')\n",
    "    full_text = ' '.join(p.get_text() for p in paragraphs)\n",
    "\n",
    "    themes = []\n",
    "    occurences = 0\n",
    "\n",
    "    for match in re.finditer(rf'\\b{re.escape(word)}\\b', full_text, re.IGNORECASE):\n",
    "        occurences += 1\n",
    "        x = match.start()\n",
    "        y = match.end()\n",
    "        \n",
    "        # On selectionne quelques mots qui entourent le mot-clé \n",
    "        sentence = full_text[max(0, x - 70): y]\n",
    "\n",
    "        # On supprime les références wikipedia eventuellement contenue\n",
    "        sentence = re.sub(r'\\[\\d+\\]', '', sentence)  \n",
    "\n",
    "        print(\"> Phrase séléctionnée: \", sentence.strip())\n",
    "\n",
    "        # On selectionne uniquement le fragment de mots contenus autour de notre mot-clé et \n",
    "        # entre 2 signes de ponctuations si presents\n",
    "        fragments = re.split(r'[.?!;,:\\n]', sentence)\n",
    "        fragment = next((frag.strip() for frag in fragments if word.lower() in frag.lower()), None)\n",
    "        print(\"> Fragment: \",fragment)\n",
    "\n",
    "        # On utilise spaCy pour supprimer les \"stopwords\" \n",
    "        doc = nlp(fragment)\n",
    "        without_stopwords = [token.text for token in doc if not token.is_stop]\n",
    "        sentence = ' '.join(without_stopwords)\n",
    "        print(\"> Fragment sans stopwords:\", sentence)\n",
    "\n",
    "        # On extrait le theme, en utilisant spaCy\n",
    "        for mot in doc:\n",
    "            if mot.text.lower() == word.lower():\n",
    "                mots_lies = []\n",
    "                i = mot.i - 1\n",
    "                while i >= 0:\n",
    "                    prev = doc[i]\n",
    "                    if prev.dep_ in (\"amod\", \"compound\") and prev.pos_ in (\"ADJ\", \"NOUN\"):\n",
    "                        mots_lies.insert(0, prev.text)\n",
    "                        i -= 1\n",
    "                    else:\n",
    "                        break\n",
    "\n",
    "                theme = ' '.join(w.lower() for w in mots_lies + [mot.text])\n",
    "                print(\"> Theme: \", theme)\n",
    "                themes.append(theme)\n",
    "                break  \n",
    "\n",
    "    return themes\n",
    "\n",
    "\n",
    "\n",
    "def wikidata_to_wikipedia(wikidata_url, lang='en'):\n",
    "    entity_id = wikidata_url.strip().split('/')[-1]\n",
    "    api_url = 'https://www.wikidata.org/w/api.php'\n",
    "    params = {'action': 'wbgetentities', 'ids': entity_id, 'format': 'json', 'props': 'sitelinks'}\n",
    "\n",
    "    response = requests.get(api_url, params=params)\n",
    "    data = response.json()\n",
    "\n",
    "    sitelinks = data['entities'].get(entity_id, {}).get('sitelinks', {})\n",
    "    wiki_key = f'{lang}wiki'\n",
    "    if wiki_key in sitelinks:\n",
    "        title = sitelinks[wiki_key].get('title')\n",
    "        if title:\n",
    "            encoded_title = urllib.parse.quote(title.replace(' ', '_'))\n",
    "            return f'https://{lang}.wikipedia.org/wiki/{encoded_title}'\n",
    "    return f\"Non trouvee.\"\n",
    "\n",
    "\n",
    "#theme(\"https://en.wikipedia.org/wiki/Rapha%C3%ABl_Coleman\",\"activist\")\n",
    "#wikidata_to_wikipedia(\"http://www.wikidata.org/entity/Q240573\", lang='en')\n",
    "\n",
    "# G1 = nx.Graph()\n",
    "# artistes = json.loads(pathlib.Path(\"artistes.json\").read_text())\n",
    "with open(\"artistes.json\", \"r\", encoding=\"utf-8\") as f:artistes = json.load(f)\n",
    "# for artiste in artistes:\n",
    "#     nom = artiste.get(\"personLabel\")\n",
    "#     wikidata_url = artiste.get(\"person\")\n",
    "#     wikipedia_url = wikidata_to_wikipedia(wikidata_url)\n",
    "#     if wikipedia_url.startswith(\"http\"):\n",
    "#         themes_extraits = theme(wikipedia_url, \"activist\")\n",
    "#         if themes_extraits:\n",
    "#             G1.add_node(nom, bipartite=0)\n",
    "#             for t in themes_extraits:\n",
    "#                 G1.add_node(t, bipartite=1)\n",
    "#                 G1.add_edge(nom, t)\n",
    "\n",
    "\n",
    "# left_nodes = {n for n, d in G1.nodes(data=True) if d.get(\"bipartite\") == 0}\n",
    "# right_nodes = {n for n, d in G1.nodes(data=True) if d.get(\"bipartite\") == 1}\n",
    "# pos = nx.bipartite_layout(G1, left_nodes)\n",
    "# plt.figure(figsize=(14, 10))\n",
    "# nx.draw_networkx(\n",
    "#     G1, pos,\n",
    "#     node_color=[\"skyblue\" if n in left_nodes else \"lightgreen\" for n in G1.nodes()],\n",
    "#     node_size=1200,\n",
    "#     font_size=9,\n",
    "#     with_labels=True,\n",
    "#     edge_color=\"gray\"\n",
    "# )\n",
    "# plt.show()\n",
    "\n",
    "# for theme in right_nodes:\n",
    "#     connected_artists = list(G1.neighbors(theme))\n",
    "#     print(f\"Le thème '{theme}' est relié à {len(connected_artists)} artiste(s)\")\n",
    "\n",
    "\n",
    "communautes = {\"climate\" :[\"climate activist\", \"environmental activist\", \"climate change activist\", \"anti - nuclear activist\"],\n",
    "               \"animal rights\":[\"animal rights activist\", \"vegan activist\"],\n",
    "                \"human rights\" : [\"human rights activist\", \"civil rights activist\"],\n",
    "                \"women & lgbt rights\" : [\"feminist activist\", \"gay activist\", \"lesbian activist\", \"transgender activist\", \"sexual purity activist\"],\n",
    "                \"political\" : [\"political activist\"],\n",
    "                \"social\":[\"social activist\", \"social media activist\", \"yellow vest activist\"],\n",
    "                \"health\" : [\"cancer research activist\"]}\n",
    "\n",
    "# G2 = nx.Graph()\n",
    "# artiste_to_themes = {}\n",
    "# for artiste in artistes:\n",
    "#     nom = artiste.get(\"personLabel\")\n",
    "#     wikidata_url = artiste.get(\"person\")\n",
    "#     wikipedia_url = wikidata_to_wikipedia(wikidata_url)\n",
    "#     if wikipedia_url.startswith(\"http\"):\n",
    "#         themes_extraits = theme(wikipedia_url, \"activist\")\n",
    "#         if themes_extraits:\n",
    "#             artiste_ajoute = False\n",
    "#             for t in themes_extraits:\n",
    "#                 for communaute, theme2 in communautes.items():\n",
    "#                     for i in range (len(theme2)):    \n",
    "#                         if theme2[i] in t: \n",
    "#                             if artiste_ajoute == False:\n",
    "#                                 G2.add_node(nom, bipartite=0) \n",
    "#                                 artiste_ajoute = True\n",
    "#                                 G2.add_node(communaute, bipartite=1)\n",
    "#                                 G2.add_edge(nom, communaute)\n",
    "#                                 artiste_to_themes.setdefault(nom, set()).add(communaute)\n",
    "#                                 break\n",
    "#                             else:\n",
    "#                                 G2.add_node(communaute, bipartite=1)\n",
    "#                                 G2.add_edge(nom, communaute)\n",
    "#                                 artiste_to_themes.setdefault(nom, set()).add(communaute)\n",
    "#                                 break\n",
    "\n",
    "\n",
    "# left_nodes = {n for n, d in G2.nodes(data=True) if d.get(\"bipartite\") == 0}\n",
    "# right_nodes = {n for n, d in G2.nodes(data=True) if d.get(\"bipartite\") == 1}\n",
    "# pos = nx.bipartite_layout(G2, left_nodes, align='vertical', scale=5)\n",
    "# plt.figure(figsize=(14, 10))\n",
    "# nx.draw_networkx(\n",
    "#     G2, pos,\n",
    "#     node_color=[\"skyblue\" if n in left_nodes else \"lightgreen\" for n in G2.nodes()],\n",
    "#     node_size=800,\n",
    "#     font_size=7,\n",
    "#     with_labels=True,\n",
    "#     edge_color=\"gray\"\n",
    "# )\n",
    "# plt.show()\n",
    "\n",
    "# with open(\"chanteurs.json\", \"r\", encoding=\"utf-8\") as f:chanteurs = json.load(f)\n",
    "# chanteurs_activists = {}\n",
    "# for personne in artiste_to_themes:\n",
    "#     for chanteur in chanteurs:\n",
    "#         if personne == chanteur.get(\"personLabel\"):\n",
    "#             chanteurs_activists[personne] = list(artiste_to_themes[personne])\n",
    "# print (chanteurs_activists)\n",
    "\n",
    "def related_words(word):\n",
    "    related = set()\n",
    "    for syn in wn.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            related.add(lemma.name())\n",
    "        for hyper in syn.hypernyms():\n",
    "            for lemma in hyper.lemmas():\n",
    "                related.add(lemma.name())\n",
    "        for hypo in syn.hyponyms():\n",
    "            for lemma in hypo.lemmas():\n",
    "                related.add(lemma.name())\n",
    "    return related\n",
    "\n",
    "# On commence maintenant l'analyse des chansons \n",
    "import lyricsgenius\n",
    "genius = lyricsgenius.Genius(\"Fu9F4P_Zft__NzCvberMcG773LOGTEU1bpqA9cdGErZYU41AWnwlmlr649k55FRD\", timeout=15)\n",
    "genius.remove_section_headers = True # Remove section headers (e.g. [Chorus]) from lyrics when searching\n",
    "genius.excluded_terms = [\"(Remix)\", \"(Live)\", \"(Edit)\", \"(Version)\"] # Exclude songs with these words in their title\n",
    "\n",
    "#pour obtenir les paroles d'une chanson\n",
    "def paroles(chanson, chanteur):\n",
    "    song = genius.search_song(chanson, chanteur)\n",
    "    return song.lyrics\n",
    "\n",
    "#pour obtenir les titres des chansons d'un artiste\n",
    "def chansons(chanteur, max_chansons):\n",
    "    artiste = genius.search_artist(chanteur, max_songs=max_chansons, sort=\"popularity\")\n",
    "    return [chanson.title for chanson in artiste.songs]\n",
    "\n",
    "def texte_paroles(texte):\n",
    "    texte = texte.lower()\n",
    "    texte = texte.replace('-', '')\n",
    "    texte = texte.translate(str.maketrans('', '', string.punctuation))\n",
    "    return set(word_tokenize(texte))\n",
    "\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "model_path = \"GoogleNews-vectors-negative300.bin\"\n",
    "model = KeyedVectors.load_word2vec_format(model_path, binary=True,limit=500000)\n",
    "\n",
    "def nettoyer_mot(mot):\n",
    "    mot = mot.replace(\"_\", \" \")          \n",
    "    mot = mot.lower()                     \n",
    "    mot = re.sub(r\"[^\\w\\s]\", \"\", mot)     \n",
    "    return mot.strip()\n",
    "\n",
    "def related_words2(keyword, seuil=0.3):\n",
    "    try:\n",
    "        similar = model.most_similar(keyword, topn=5000000)  \n",
    "        mots = set([word for word, score in similar if score >= seuil])\n",
    "        return set(nettoyer_mot(mot) for mot in mots)\n",
    "    except KeyError:\n",
    "        return set()\n",
    "    \n",
    "\n",
    "#attribue un score de similarité entre les paroles d'une chanson et un theme\n",
    "def score_similarite(chanson, chanteur, theme):\n",
    "    mots_lies = related_words2(theme)\n",
    "    lyrics = paroles(chanson, chanteur)\n",
    "    mots_lyrics = texte_paroles(lyrics)\n",
    "    mots_communs = mots_lyrics.intersection(mots_lies)\n",
    "    print(mots_communs)\n",
    "    #return len(mots_communs) / max(1, len(mots_lyrics))\n",
    "    if len(mots_communs) >= 20:\n",
    "        return \"A+\"\n",
    "    if 15 <= len(mots_communs) <= 20:\n",
    "        return \"A\"\n",
    "    if 10<=len(mots_communs) <= 15:\n",
    "        return \"B\"\n",
    "    if 5<= len(mots_communs) <= 10:\n",
    "        return \"C\"\n",
    "    if 0 < len (mots_communs)<= 5:\n",
    "        return \"D\"\n",
    "    else: \n",
    "        return \"E\"\n",
    "\n",
    "chanteurs_themes = {'M.I.A.': ['politics'], 'KRS-One': ['human rights', 'animal rights'], 'Killer Mike': ['social'], 'Michael Cuccione': ['health'], 'L7a9d': ['human rights'], 'Saba Saba': ['social'], 'Rebecca Moore': ['animal rights'], 'Joaquin Phoenix': ['animal rights'], 'Wyclef Jean': ['politics'], 'Adé Bantu': ['social'], 'Tupac Shakur': ['politics', 'human rights'], 'Mai Khôi': ['politics'], 'Jacline Mouraud': ['social'], 'Madeleina Kay': ['politics'], 'Topher': ['politics'], 'Montana Tucker': ['social']}\n",
    "# for chanteur, themes_associes in chanteurs_themes.items():\n",
    "#      songs = chansons(chanteur, 5)  \n",
    "#      for theme in themes_associes:\n",
    "#          scores = []\n",
    "#          for chanson in songs:\n",
    "#              score_chanson = score_similarite(chanson, chanteur, theme)\n",
    "#              scores.append(score_chanson)\n",
    "#          print (chanteur, theme, scores)\n",
    "\n",
    "\n",
    "mots = [\"pollution\", \"forest\", \"recycling\", \"plastic\", \"energy\", \"biodiversity\", \"climate\", \"oil\",\"deforestation\", \"carbon\", \"agriculture\", \"ocean\", \"sustainable\", \"windmill\", \"compost\",\"glacier\", \"solar\", \"emission\", \"development\", \"hurricane\", \"car\", \"computer\", \"phone\",\"pizza\", \"football\", \"music\", \"school\", \"mountain\", \"internet\", \"coffee\"]\n",
    "\n",
    "def test_seuil(mots, seuil, theme):\n",
    "    try:\n",
    "        similar = model.most_similar(theme, topn=5000000)  \n",
    "        m = set([word for word, score in similar if score >= seuil])\n",
    "        s = set(nettoyer_mot(mot) for mot in m)\n",
    "    except KeyError:\n",
    "        s = set()\n",
    "    mots = set(mots)\n",
    "    mots_communs = mots.intersection(s)\n",
    "    print (mots_communs)\n",
    "    return len (mots_communs)\n",
    "\n",
    "# seuils = np.arange(0, 1.1, 0.1)\n",
    "# longueurs = [test_seuil(mots, seuil, \"environmental\") for seuil in seuils]\n",
    "# plt.plot(seuils, longueurs, marker='o')\n",
    "# plt.title(\"Nombre de mots détectésen fonction du seuil\")\n",
    "# plt.xlabel(\"Seuil de similarité\")\n",
    "# plt.ylabel(\"Nombre de mots en commun\")\n",
    "# plt.grid(True)\n",
    "# plt.show()\n",
    "\n",
    "def nationalite(url):\n",
    "    if not url or not url.startswith(\"http\"):\n",
    "        return \"invalid url\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        return \"error\"\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    infobox = soup.find(\"table\", class_=\"infobox\")\n",
    "    if not infobox:\n",
    "        return \"no infobox found\"\n",
    "    for row in infobox.find_all(\"tr\"):\n",
    "        header = row.find(\"th\")\n",
    "        if header and \"nationality\" in header.get_text(strip=True).lower():\n",
    "            data = row.find(\"td\")\n",
    "            if data:\n",
    "                return data.get_text(strip=True)\n",
    "    for row in infobox.find_all(\"tr\"):\n",
    "        header = row.find(\"th\")\n",
    "        if header and \"born\" in header.get_text(strip=True).lower():\n",
    "            data = row.find(\"td\")\n",
    "            if data:\n",
    "                lines = data.get_text(separator=\"\\n\", strip=True).split(\"\\n\")\n",
    "                if lines:\n",
    "                    last_line = lines[-1].strip()\n",
    "                    if ',' in last_line:\n",
    "                        parts = [p.strip() for p in last_line.split(',')]\n",
    "                        return parts[-1]\n",
    "                    return last_line\n",
    "    return \"not found\"\n",
    "\n",
    "with open(\"chanteurs.json\", \"r\", encoding=\"utf-8\") as f:chanteurs = json.load(f)\n",
    "noms_filtres = set(chanteurs_themes.keys())\n",
    "\n",
    "# chanteurs_nationalite = {}\n",
    "\n",
    "# for chanteur in chanteurs:\n",
    "#     nom = chanteur.get(\"personLabel\")\n",
    "#     if nom in noms_filtres:\n",
    "#         wikidata_url = chanteur.get(\"person\")\n",
    "#         wikipedia_url = wikidata_to_wikipedia(wikidata_url)\n",
    "#         if not wikipedia_url or not wikipedia_url.startswith(\"http\"):\n",
    "#             pays = \"invalid url\"\n",
    "#         else:\n",
    "#             pays = nationalite(wikipedia_url)\n",
    "#         chanteurs_nationalite[nom] = pays\n",
    "# print(chanteurs_nationalite)\n",
    "\n",
    "def genres(url):\n",
    "    if not url or not url.startswith(\"http\"):\n",
    "        return \"invalid url\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        return \"error\"\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    infobox = soup.find(\"table\", class_=\"infobox\")\n",
    "    if not infobox:\n",
    "        return \"no infobox found\"\n",
    "    for row in infobox.find_all(\"tr\"):\n",
    "        header = row.find(\"th\")\n",
    "        if header and \"genres\" in header.get_text(strip=True).lower():\n",
    "            data = row.find(\"td\")\n",
    "            if data:\n",
    "                return data.get_text(separator=\", \", strip=True)\n",
    "    return \"not found\"\n",
    "\n",
    "# chanteurs_genres = {}\n",
    "\n",
    "# for chanteur in chanteurs:\n",
    "#     nom = chanteur.get(\"personLabel\")\n",
    "#     if nom in noms_filtres:\n",
    "#         wikidata_url = chanteur.get(\"person\")\n",
    "#         wikipedia_url = wikidata_to_wikipedia(wikidata_url)\n",
    "#         if not wikipedia_url or not wikipedia_url.startswith(\"http\"):\n",
    "#             genre = \"invalid url\"\n",
    "#         else:\n",
    "#             genre = genres(wikipedia_url)\n",
    "#         chanteurs_genres[nom] = genre\n",
    "\n",
    "# chanteurs_genres2 = {}\n",
    "\n",
    "# # Nettoyer la liste obtenue et ne laisser que les artistes qui possèdent des genres\n",
    "# for artist, genres in chanteurs_genres.items():\n",
    "#     if genres == \"not found\" or not genres:\n",
    "#         continue  \n",
    "#     cleaned = re.sub(r\"[\\[\\]0-9/]\", \"\", genres)\n",
    "#     cleaned = re.sub(r\",\\s*,+\", \",\", cleaned)  \n",
    "#     cleaned = re.sub(r\"\\s+\", \" \", cleaned) \n",
    "#     cleaned = [g.strip().lower() for g in cleaned.split(\",\") if g.strip()]\n",
    "#     chanteurs_genres2[artist] = list(set(cleaned)) \n",
    "# print (chanteurs_genres2)\n",
    "\n",
    "with open(\"femmes.json\", \"r\", encoding=\"utf-8\") as f:femmes = json.load(f)\n",
    "\n",
    "# chanteurs_gender = {}\n",
    "\n",
    "# for chanteur in chanteurs:\n",
    "#     nom = chanteur.get(\"personLabel\")\n",
    "#     if nom in noms_filtres:\n",
    "#         if chanteur in femmes:\n",
    "#             chanteurs_gender[nom] = \"femme\"\n",
    "#         else:\n",
    "#             chanteurs_gender[nom] = \"homme\"\n",
    "\n",
    "def naissance(url):\n",
    "    if not url or not url.startswith(\"http\"):\n",
    "        return \"invalid url\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        return \"error\"\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    infobox = soup.find(\"table\", class_=\"infobox\")\n",
    "    if not infobox:\n",
    "        return \"no infobox found\"\n",
    "    rows = infobox.find_all(\"tr\")\n",
    "    for row in rows:\n",
    "        header = row.find(\"th\")\n",
    "        if header and \"born\" in header.text.lower():\n",
    "            born_cell = row.find(\"td\")\n",
    "            if born_cell:\n",
    "                text = born_cell.get_text(\" \", strip=True)\n",
    "                match = re.search(r\"\\b(19[6-9][0-9]|20[0-9]{2})\\b\", text)\n",
    "                if match:\n",
    "                    return int(match.group(1))\n",
    "                else:\n",
    "                    return \"no valid year found\"\n",
    "    return \"no 'born' field found\"\n",
    "    \n",
    "# chanteurs_naissance = {}\n",
    "# for chanteur in chanteurs:\n",
    "#     nom = chanteur.get(\"personLabel\")\n",
    "#     if nom in noms_filtres:\n",
    "#         wikidata_url = chanteur.get(\"person\")\n",
    "#         wikipedia_url = wikidata_to_wikipedia(wikidata_url)\n",
    "#         if not wikipedia_url or not wikipedia_url.startswith(\"http\"):\n",
    "#             annee = \"invalid url\"\n",
    "#         else:\n",
    "#             annee = naissance(wikipedia_url)\n",
    "#         chanteurs_naissance[nom] = annee\n",
    "# print (chanteurs_naissance)\n",
    "\n",
    "chanteurs_gender2 = {'KRS-One': 'homme', 'Tupac Shakur': 'homme', 'Adé Bantu': 'homme', 'Joaquin Phoenix': 'homme', 'Wyclef Jean': 'homme', 'Killer Mike': 'homme', 'M.I.A.': 'femme', 'Saba Saba': 'homme', 'Mai Khôi': 'femme', 'Michael Cuccione': 'homme', 'L7a9d': 'homme', 'Rebecca Moore': 'femme', 'Madeleina Kay': 'femme', 'Jacline Mouraud': 'femme', 'Topher': 'homme', 'Montana Tucker': 'femme'}    \n",
    "chanteurs_nationalite2 =  {'KRS-One': 'U.S.', 'Tupac Shakur': 'U.S.', 'Adé Bantu': 'England', 'Joaquin Phoenix': 'Puerto Rico', 'Wyclef Jean': 'Haiti', 'Killer Mike': 'U.S.', 'M.I.A.': 'England', 'Saba Saba': 'Uganda', 'Mai Khôi': 'Vietnam', 'Michael Cuccione': 'Canada', 'L7a9d': 'Morocco', 'Rebecca Moore': 'U.S.', 'Madeleina Kay': 'England', 'Jacline Mouraud': 'France', 'Topher': 'U.S.', 'Montana Tucker': 'U.S.'}\n",
    "chanteurs_genres3 = {'KRS-One': ['conscious rap', 'hardcore hip-hop', 'political hip-hop', 'east coast hip-hop'], 'Tupac Shakur': ['political hip-hop', 'west coast hip-hop', 'gangsta rap'], 'Adé Bantu': ['hip-hop', 'afrofunk', 'fuji', 'afrobeat'], 'Wyclef Jean': ['east coast hip hop', 'neo soul', 'r&b', 'reggae fusion', 'pop rap'], 'Killer Mike': ['southern hip-hop', 'gospel'], 'M.I.A.': ['hip hop', 'alternative hip hop', 'world', 'dance', 'progressive rap', 'pop', 'electronica', 'experimental'], 'Saba Saba': ['hip hop', 'african hip hop'], 'Mai Khôi': ['pop'], 'L7a9d': ['rap'], 'Rebecca Moore': ['alternative', 'ambient', 'experimental'], 'Topher': ['hip hop']}\n",
    "chanteurs_naissance2 = {'KRS-One': 1965, 'Tupac Shakur': 1971, 'Adé Bantu': 1971, 'Joaquin Phoenix': 1974, 'Wyclef Jean': 1969, 'Killer Mike': 1975, 'M.I.A.': 1975, 'Saba Saba': 1977, 'Mai Khôi': 1983, 'Michael Cuccione': 1985, 'L7a9d': 1988, 'Rebecca Moore': 1968, 'Madeleina Kay': 1994, 'Jacline Mouraud': 1967, 'Topher': 1991, 'Montana Tucker': 1993}\n",
    "\n",
    "chanteurs = chanteurs_gender2.keys()\n",
    "print (chanteurs)\n",
    "dictionnaire = [\n",
    "    {'nationalite': chanteurs_nationalite2.get(chanteur), 'gender': chanteurs_gender2.get(chanteur), 'genres':chanteurs_genres3.get(chanteur, []), 'naissance':chanteurs_naissance2.get(chanteur)/1965}\n",
    "    for chanteur in chanteurs\n",
    "]\n",
    "print (dictionnaire)\n",
    "\n",
    "vec= DictVectorizer()\n",
    "X = vec.fit_transform(dictionnaire).toarray()\n",
    "\n",
    "for k in range(2,len(X[0])):\n",
    "    print(\"number of clusters: \"+ str(k))\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0).fit(X)\n",
    "    print(silhouette_score(X, kmeans.labels_))\n",
    "    print(kmeans.labels_)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
